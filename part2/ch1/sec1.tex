\section{Definitions and examples}\label{ii:sec:1.1}

\begin{lem}\label{ii:1.1.1}
  Let \((x_n)_{n = m}^\infty\) be a sequence of real numbers, and let \(x\) be another real number.
  Then \((x_n)_{n = m}^\infty\) converges to \(x\) iff \(\lim_{n \to \infty} d(x_n, x) = 0\).
\end{lem}

\begin{proof}
  \begin{align*}
         & \lim_{n \to \infty} x_n = x                              \\
    \iff & \lim_{n \to \infty} x_n - x = 0                          \\
    \iff & \lim_{n \to \infty} \abs{x_n - x} = 0 &  & \by{i:6.4.17} \\
    \iff & \lim_{n \to \infty} d(x_n, x) = 0.
  \end{align*}
\end{proof}

\begin{note}
  One would now like to generalize this notion of convergence, so that one can take limits not just of sequences of real numbers, but also sequences of complex numbers, or sequences of vectors, or sequences of matrices, or sequences of functions, even sequences of sequences.
  One way to do this is to redefine the notion of convergence each time we deal with a new type of object.
  A more efficient way is to work \emph{abstractly}, defining a very general class of spaces - which includes such standard spaces as the real numbers, complex numbers, vectors, etc. - and define the notion of convergence on this entire class of spaces at once.
  (A \emph{space} is just the set of all objects of a certain type.
  Mathematically, there is not much distinction between a space and a set, except that spaces tend to have much more structure than what a random set would have.)
\end{note}

\begin{note}
  It turns out that there are two very useful classes of spaces which do the job.
  The first class is that of \emph{metric spaces}.
  There is a more general class of spaces, called \emph{topological spaces}.
\end{note}

\begin{defn}[Metric spaces]\label{ii:1.1.2}
  A metric space \((X, d)\) is a space \(X\) of objects (called \emph{points}), together with a \emph{distance function} or \emph{metric} \(d : X \times X \to [0, +\infty)\), which associates to each pair \(x, y\) of points in \(X\) a non-negative real number \(d(x, y) \geq 0\).
  Furthermore, the metric must satisfy the following four axioms:
  \begin{enumerate}
    \item For any \(x \in X\), we have \(d(x, x) = 0\).
    \item (Positivity) For any distinct \(x, y \in X\), we have \(d(x, y) > 0\).
    \item (Symmetry) For any \(x, y \in X\), we have \(d(x, y) = d(y, x)\).
    \item (Triangle inequality) For any \(x, y, z \in X\), we have \(d(x, z) \leq d(x, y) + d(y, z)\).
  \end{enumerate}
\end{defn}

\begin{note}
  In many cases it will be clear what the metric \(d\) is, and we shall abbreviate \((X, d)\) as just \(X\).
\end{note}

\begin{rmk}\label{ii:1.1.3}
  The conditions (a) and (b) of \cref{ii:1.1.1} can be rephrased as follows:
  for any \(x, y \in X\) we have \(d(x, y) = 0\) iff \(x = y\).
\end{rmk}

\begin{eg}[The real line]\label{ii:1.1.4}
  Let \(\R\) be the real numbers, and let \(d : \R \times \R \to [0, \infty)\) be the metric \(d(x, y) \coloneqq \abs{x - y}\) mentioned earlier.
  Then \((\R, d)\) is a metric space.
  We refer to \(d\) as the \emph{standard metric} on \(\R\), and if we refer to \(\R\) as a metric space, we assume that the metric is given by the standard metric \(d\) unless otherwise specified.
\end{eg}

\begin{eg}[Induced metric spaces]\label{ii:1.1.5}
  Let \((X, d)\) be any metric space, and let \(Y\) be a subset of \(X\).
  Then we can restrict the metric function \(d : X \times X \to [0, +\infty)\) to the subset \(Y \times Y\) of \(X \times X\) to create a restricted metric function \(d|_{Y \times Y} : Y \times Y \to [0, +\infty)\) of \(Y\);
  this is known as the metric on \(Y\) \emph{induced} by the metric \(d\) on \(X\).
  The pair \((Y, d|_{Y \times Y})\) is a metric space and is known the \emph{subspace} of \((X, d)\) induced by \(Y\).
  Thus, for instance the metric on the real line in the \cref{ii:1.1.4} induces a metric space structure on any subset of the reals, such as the integers \(Z\), or an interval \([a, b]\), etc.
\end{eg}

\begin{eg}[Euclidean spaces]\label{ii:1.1.6}
  Let \(n \geq 1\) be a natural number, and let \(\R^n\) be the space of \(n\)-tuples of real numbers:
  \[
    \R^n = \set{(x_1, x_2, \dots, x_n) : x_1, \dots, x_n \in \R}.
  \]
  We define the \emph{Euclidean metric} (also called the \emph{\(l^2\) metric}) \(d_{l^2} : \R^n \times\R^n \to \R\) by
  \begin{align*}
    d_{l^2}((x_1, \dots, x_n), (y_1, \dots, y_n)) & \coloneqq \sqrt{(x_1 - y_1)^2 + \dots + (x_n - y_n)^2} \\
                                                  & = \bigg(\sum_{i = 1}^n (x_i - y_i)^2\bigg)^{1 / 2}.
  \end{align*}
\end{eg}

\begin{note}
  Euclidean metric corresponds to the geometric distance between the two points \((x_1, x_2, \dots, x_n)\), \((y_1, y_2, \dots, y_n)\) as given by Pythagoras' theorem.
  While geometry does give some very important examples of metric spaces, it is possible to have metric spaces which have no obvious geometry whatsoever.
  The verification that \((\R^n, d)\) is indeed a metric space can be seen geometrically (for instance, the triangle inequality now asserts that the length of one side of a triangle is always less than or equal to the sum of the lengths of the other two sides), but can also be proven algebraically.
  We refer to \((\R^n , d_{l^2})\) as the \emph{Euclidean space} of \emph{dimension \(n\)}.
  Extending the convention from \cref{ii:1.1.4}, if we refer to \(\R^n\) as a metric space, we assume that the metric is given by the Euclidean metric unless otherwise specified.
\end{note}

\begin{eg}[Taxi-cab metric]\label{ii:1.1.7}
  Again let \(n \geq 1\), and let \(\R^n\) be as before.
  But now we use a different metric \(d_{l^1}\), the so-called \emph{taxicab metric} (or \emph{\(l^1\) metric}), defined by
  \begin{align*}
    d_{l^1}((x_1, \dots, x_n), (y_1, \dots, y_n)) & \coloneqq \abs{x_1 - y_1} + \dots + \abs{x_n - y_n} \\
                                                  & = \sum_{i = 1}^n \abs{x_i - y_i}.
  \end{align*}
\end{eg}

\begin{note}
  This metric is called the taxi-cab metric, because it models the distance a taxi-cab would have to traverse to get from one point to another if the cab was only allowed to move in cardinal directions (north, south, east, west) and not diagonally.
  As such it is always at least as large as the Euclidean metric, which measures distance ``as the crow flies,'' as it were.
  We claim that the space \((\R^n, d_{l^1})\) is also a metric space.
  The metrics are not quite the same, but we do have the inequalities
  \[
    d_{l^2}(x, y) \leq d_{l^1}(x, y) \leq \sqrt{n} d_{l^2}(x, y)
  \]
  for all \(x, y\).
\end{note}

\begin{rmk}\label{ii:1.1.8}
  The taxi-cab metric is useful in several places, for instance in the theory of error correcting codes.
  A string of \(n\) binary digits can be thought of as an element of \(\R^n\).
  The taxi-cab distance between two binary strings is then the number of bits in the two strings which do not match.
  The goal of error-correcting codes is to encode each piece of information (e.g., a letter of the alphabet) as a binary string in such a way that all the binary strings are as far away in the taxicab metric from each other as possible;
  this minimizes the chance that any distortion of the bits due to random noise can accidentally change one of the coded binary strings to another, and also maximizes the chance that any such distortion can be detected and correctly repaired.
\end{rmk}

\begin{eg}[Sup norm metric]\label{ii:1.1.9}
  Again let \(n \geq 1\), and let \(\R^n\) be as before.
  But now we use a different metric \(d_{l^\infty}\), the so-called \emph{sup norm metric} (or \emph{\(l^\infty\) metric}), defined by
  \[
    d_{l^\infty} ((x_1, \dots, x_n), (y_1, \dots, y_n)) \coloneqq \sup\set{\abs{x_i - y_i} : 1 \leq i \leq n}.
  \]
\end{eg}

\begin{note}
  The space \((\R^n, d_{l^\infty})\) is also a metric space, and is related to the \(l^2\) metric by the inequalities
  \[
    \dfrac{1}{\sqrt{n}} d_{l^2}(x, y) \leq d_{l^\infty}(x, y) \leq d_{l^2}(x, y)
  \]
  for all \(x, y\).
\end{note}

\begin{rmk}\label{ii:1.1.10}
  The \(l^1\), \(l^2\), and \(l^\infty\) metrics are special cases of the more general \emph{\(l^p\) metrics}, where \(p \in [1, +\infty)\).
\end{rmk}

\begin{eg}[Discrete metric]\label{ii:1.1.11}
  Let \(X\) be an arbitrary set (finite or infinite), and define the \emph{discrete metric} \(d_{\text{disc}}\) by setting \(d_{\text{disc}}(x, y) \coloneqq 0\) when \(x = y\), and \(d_{\text{disc}}(x, y) \coloneqq 1\) when \(x \neq y\).
  Thus, in this metric, all points are equally far apart.
  The space \((X, d_{\text{disc}})\) is a metric space.
  Thus, every set \(X\) has at least one metric on it.
\end{eg}

\setcounter{thm}{13}
\begin{defn}[Convergence of sequences in metric spaces]\label{ii:1.1.14}
  Let \(m\) be an integer, \((X, d)\) be a metric space and let \((x^{(n)})_{n = m}^\infty\) be a sequence of points in \(X\)
  (i.e., for every natural number \(n \geq m\), we assume that \(x^{(n)}\) is an element of \(X\)).
  Let \(x\) be a point in \(X\).
  We say that \emph{\((x^{(n)})_{n = m}^\infty\) converges to \(x\) with respect to the metric \(d\)}, iff the limit \(\lim_{n \to \infty} d(x^{(n)}, x)\) exists and is equal to \(0\).
  In other words, \((x^{(n)})_{n = m}^\infty\) converges to \(x\) with respect to \(d\) iff for every \(\varepsilon > 0\), there exists an \(N \geq m\) such that \(d(x^{(n)}, x) \leq \varepsilon\) for all \(n \geq N\).
\end{defn}

\begin{rmk}\label{ii:1.1.15}
  In view of \cref{ii:1.1.1} we see that this definition generalizes our existing notion of convergence of sequences of real numbers.
  In many cases, it is obvious what the metric d is, and so we shall often just say ``\((x^{(n)})_{n = m}^\infty\) converges to \(x\)'' instead of ``\((x^{(n)})_{n = m}^\infty\) converges to \(x\) with respect to the metric \(d\)'' when there is no chance of confusion.
  We also sometimes write ``\(x^{(n)} \to x\) as \(n \to \infty\) instead.
\end{rmk}

\begin{rmk}\label{ii:1.1.16}
  There is nothing special about the superscript \(n\) in the above definition;
  it is a dummy variable.
  Saying that \((x^{(n)})_{n = m}^\infty\) converges to \(x\) is exactly the same statement as saying that \((x^{(k)})_{k = m}^\infty\) converges to \(x\), for example;
  and sometimes it is convenient to change superscripts, for instance if the variable \(n\) is already being used for some other purpose.
  Similarly, it is not necessary for the sequence \(x^{(n)}\) to be denoted using the superscript \((n)\);
  the above definition is also valid for sequences \(x_n\), or functions \(f(n)\), or indeed of any expression which depends on \(n\) and takes values in \(X\).
  We see that the starting point \(m\) of the sequence is unimportant for the purposes of taking limits;
  if \((x^{(n)})_{n = m}^\infty\) converges to \(x\), then \((x^{(n)})_{n = m'}^\infty\) also converges to \(x\) for any \(m' \geq m\).
\end{rmk}

\begin{note}
  The convergence of a sequence can depend on what metric one uses.
\end{note}

\setcounter{thm}{17}
\begin{prop}[Equivalence of \(l^1\), \(l^2\), \(l^\infty\)]\label{ii:1.1.18}
  Let \(\R^n\) be a Euclidean space, and let \((x^{(k)})_{k = m}^\infty\) be a sequence of points in \(\R^n\).
  We write \(x^{(k)} = (x_1^{(k)}, x_2^{(k)}, \dots, x_n^{(k)})\), i.e., for \(j = 1, 2, \dots, n\), \(x_j \in \R\) is the \(j^{\opTh}\) coordinate of \(x^{(k)} \in \R^n\).
  Let \(x = (x_1, \dots, x_n)\) be a point in \(\R^n\).
  Then the following four statements are equivalent:
  \begin{enumerate}
    \item \((x^{(k)})_{k = m}^\infty\) converges to \(x\) with respect to the Euclidean metric \(d_{l^2}\).
    \item \((x^{(k)})_{k = m}^\infty\) converges to \(x\) with respect to the taxi-cab metric \(d_{l^1}\).
    \item \((x^{(k)})_{k = m}^\infty\) converges to \(x\) with respect to the sup norm metric \(d_{l^\infty}\).
    \item For every \(1 \leq j \leq n\), the sequence \((x_j^{(k)})_{k = m}^\infty\) converges to \(x_j\).
          (Notice that this is a sequence of real numbers, not of points in \(\R^n\).)
  \end{enumerate}
\end{prop}

\begin{proof}
  We have
  \begin{align*}
         & \lim_{k \to \infty} d_{l^2}(x^{(k)}, x) = 0                                                      &  & \by{ii:1.1.14} \\
    \iff & \lim_{k \to \infty} \sqrt{\sum_{j = 1}^n (x_j^{(k)} - x_j)^2} = 0                                &  & \by{ii:1.1.6}  \\
    \iff & \lim_{k \to \infty} \bigg(\sum_{j = 1}^n (x_j^{(k)} - x_j)^2\bigg) = 0                                               \\
    \iff & \sum_{j = 1}^n \bigg(\lim_{k \to \infty} (x_j^{(k)} - x_j)^2\bigg) = 0                                               \\
    \iff & \forall j \in \set{i \in \N : 1 \leq i \leq n}, \lim_{k \to \infty} x_j^{(k)} - x_j = 0                              \\
    \iff & \forall j \in \set{i \in \N : 1 \leq i \leq n}, \lim_{k \to \infty} x_j^{(k)} = x_j                                  \\
    \iff & \forall j \in \set{i \in \N : 1 \leq i \leq n}, \lim_{k \to \infty} \abs{x_j^{(k)} - x_j} = 0    &  & \by{ii:1.1.1}  \\
    \iff & \sum_{j = 1}^n \bigg(\lim_{k \to \infty} \abs{x_j^{(k)} - x_j}\bigg) = 0                                             \\
    \iff & \lim_{k \to \infty} \bigg(\sum_{j = 1}^n \abs{x_j^{(k)} - x_j}\bigg) = 0                                             \\
    \iff & \lim_{k \to \infty} d_{l^1}(x^{(k)}, x) = 0                                                      &  & \by{ii:1.1.7}  \\
    \iff & \lim_{k \to \infty} \sup\set{\abs{x_j^{(k)} - x_j} : j \in \set{i \in \N : 1 \leq i \leq n}} = 0                     \\
    \iff & \lim_{k \to \infty} d_{l^\infty}(x^{(k)}, x) = 0.                                                &  & \by{ii:1.1.9}
  \end{align*}
\end{proof}

\begin{note}
  Because of the equivalence of \cref{ii:1.1.18}(a), (b) and (c), we say that the Euclidean, taxicab, and sup norm metrics on \(\R^n\) are \emph{equivalent}.
  (There are infinite-dimensional analogues of the Euclidean, taxicab, and sup norm metrics which are \emph{not} equivalent.)
\end{note}

\begin{prop}[Convergence in the discrete metric]\label{ii:1.1.19}
  Let \(X\) be any set, and let \(d_{\text{disc}}\) be the discrete metric on \(X\).
  Let \((x^{(n)})_{n = m}^\infty\) be a sequence of points in \(X\), and let \(x\) be a point in \(X\).
  Then \((x^{(n)})_{n = m}^\infty\) converges to \(x\) with respect to the discrete metric \(d_{\text{disc}}\) iff there exists an \(N \geq m\) such that \(x^{(n)} = x\) for all \(n \geq N\).
\end{prop}

\begin{proof}
  By \cref{ii:1.1.14} we know that \(\lim_{n \to \infty} d_{\text{disc}}(x^{(n)}, x) = 0\) iff \(\forall \varepsilon \in \R^+\), \(\exists N \in \N\) and \(N \geq m\) such that \(d(x^{(n)}, x) \leq \varepsilon\) for all \(n \geq N\).
  By \cref{ii:1.1.11} we know that \(d_{\text{disc}}(x^{(n)}, x) \leq \varepsilon\) iff \(x^{(n)} = x\).
\end{proof}

\begin{prop}[Uniqueness of limits]\label{ii:1.1.20}
  Let \((X, d)\) be a metric space, and let \((x^{(n)})_{n = m}^\infty\) be a sequence in \(X\).
  Suppose that there are two points \(x, x' \in X\) such that \((x^{(n)})_{n = m}^\infty\) converges to \(x\) with respect to \(d\), and \((x^{(n)})_{n = m}^\infty\) also converges to \(x'\) with respect to \(d\).
  Then we have \(x = x'\).
\end{prop}

\begin{proof}
  By \cref{ii:1.1.14} we have \(\lim_{n \to \infty} d(x^{(n)}, x) = 0\) and \(\lim_{n \to \infty} d(x^{(n)}, x') = 0\).
  So
  \begin{align*}
             & \lim_{n \to \infty} d(x^{(n)}, x) + \lim_{n \to \infty} d(x^{(n)}, x') = 0                         \\
    \implies & \lim_{n \to \infty} \bigg(d(x^{(n)}, x) + d(x^{(n)}, x')\bigg) = 0                                 \\
    \implies & d(x, x') = \lim_{n \to \infty} d(x, x') \leq 0                             &  & \by{ii:1.1.2}[d]   \\
    \implies & d(x, x') = 0                                                               &  & \by{ii:1.1.2}[a,b] \\
    \implies & x = x'.                                                                    &  & \by{ii:1.1.2}[a]
  \end{align*}
\end{proof}

\begin{note}
  Because of \cref{ii:1.1.20}, it is safe to introduce the following notation:
  if \((x^{(n)})_{n = m}^\infty\) converges to \(x\) in the metric \(d\), then we write \(d - \lim_{n \to \infty} x^{(n)} = x\), or simply \(\lim_{n \to \infty} x^{(n)} = x\) when there is no confusion as to what \(d\) is.
  The meaning of \(d - \lim_{n \to \infty} x^{(n)}\) can depend on what \(d\) is;
  however \cref{ii:1.1.20} assures us that once \(d\) is fixed, there can be at most one value of \(d - \lim_{n \to \infty} x^{(n)}\).
  (Of course, it is still possible that this limit does not exist;
  some sequences are not convergent.)
\end{note}

\begin{rmk}\label{ii:1.1.21}
  It is possible for a sequence to converge to one point using one metric, and another point using a different metric, although such examples are usually quite artificial.
  Thus, changing the metric on a space can greatly affect the nature of convergence (also called the \emph{topology}) on that space.
\end{rmk}

\begin{ac}\label{ii:ac:1.1.1}
  Let \(\vec{u} = (u_1, u_2), \vec{v} = (v_1, v_2)\) be two vectors in \(\R^2\) such that \(\abs{\vec{v}} \neq 0\).
  Let \(\alpha \in \R\) and let \(\alpha \vec{v}\) be the vector \(\vec{u}\) project onto \(\vec{v}\).
  Then
  \[
    \alpha = \dfrac{\vec{u} \cdot \vec{v}}{\abs{\vec{v}}^2}.
  \]
\end{ac}

\begin{proof}
  Let \(\vec{z} = (v_2, -v_1)\).
  We know that \(\vec{v} \bot \vec{z}\) since
  \[
    \vec{v} \cdot \vec{z} = v_1 v_2 - v_1 v_2 = 0.
  \]
  Since \(\alpha \vec{v}\) is the vector \(\vec{u}\) project onto \(\vec{v}\), we know that \(\exists \beta \in \R\) such that \(\alpha \vec{v} + \beta \vec{z} = \vec{u}\).
  Then we have
  \begin{align*}
             & \alpha \vec{v} + \beta \vec{z} = \vec{u}                                                            \\
    \implies & \begin{dcases}
                 \alpha v_1 + \beta v_2 = u_1 \\
                 \alpha v_2 - \beta v_1 = u_2
               \end{dcases}                                                                        \\
    \implies & \begin{dcases}
                 \alpha v_1^2 + \beta v_1 v_2 = u_1 v_1 \\
                 \alpha v_2^2 - \beta v_1 v_2 = u_2 v_2
               \end{dcases}                                                              \\
    \implies & \alpha (v_1^2 + v_2^2) = u_1 v_1 + u_2 v_2                                                          \\
    \implies & \alpha = \dfrac{u_1 v_1 + u_2 v_2}{v_1^2 + v_2^2} = \dfrac{\vec{u} \cdot \vec{v}}{\abs{\vec{v}}^2}.
  \end{align*}
\end{proof}

\begin{ac}\label{ii:ac:1.1.2}
  Let \(n \in \Z^+\) and let \(\vec{u}, \vec{v}\) be two vectors in \(\R^n\) such that \(\abs{\vec{v}} \neq 0\).
  Let \(\alpha \in \R\) and let \(\alpha \vec{v}\) be the vector \(\vec{u}\) project onto \(\vec{v}\).
  Then
  \[
    \alpha = \dfrac{\vec{u} \cdot \vec{v}}{\abs{\vec{v}}^2}.
  \]
\end{ac}

\begin{proof}
  We can use \(\vec{u}\) and \(\vec{v}\) to form a linear combination \(\vec{z} \in \R^n\) such that \(\vec{z} \bot \vec{v}\).
  Since \(\alpha \vec{v}\) is the vector \(\vec{u}\) project onto \(\vec{v}\), we know that \(\exists \beta \in \R\) such that \(\alpha \vec{v} + \beta \vec{z} = \vec{u}\).
  Then we have \(\beta \vec{z} \cdot \vec{v} = 0\) and
  \begin{align*}
             & \alpha \vec{v} + \beta \vec{z} = \vec{u}                                                                        \\
    \implies & \alpha \vec{v} \cdot \vec{v} + \beta \vec{z} \cdot \vec{v} = \vec{u} \cdot \vec{v}                              \\
    \implies & \alpha = \dfrac{\vec{u} \cdot \vec{v}}{\vec{v} \cdot \vec{v}} = \dfrac{\vec{u} \cdot \vec{v}}{\abs{\vec{v}}^2}.
  \end{align*}
\end{proof}

\exercisesection

\begin{ex}\label{ii:ex:1.1.1}
  Prove \cref{ii:1.1.1}.
\end{ex}

\begin{proof}
  See \cref{ii:1.1.1}.
\end{proof}

\begin{ex}\label{ii:ex:1.1.2}
  Show that the real line with the metric \(d(x, y) \coloneqq \abs{x - y}\) is indeed a metric space.
\end{ex}

\begin{proof}
  Let \(x, y, z \in \R\).
  For identity:
  We have \(d(x, x) = \abs{x - x} = 0\).
  For positivity:
  If \(x \neq y\), then \(d(x, y) = \abs{x - y} > 0\).
  For symmetry:
  We have \(d(x, y) = \abs{x - y} = \abs{y - x} = d(y, x)\).
  For triangle inequality:
  We have \(d(x, z) = \abs{x - z} = \abs{x - y + y - z} \leq \abs{x - y} + \abs{y - z} = d(x, y) + d(y, z)\).
  Thus, by \cref{ii:1.1.2} \((\R, d)\) is a metric space.
\end{proof}

\begin{ex}\label{ii:ex:1.1.3}
  Let \(X\) be a set, and let \(d : X \times X \to [0, \infty)\) be a function.
  \begin{enumerate}
    \item Give an example of a pair \((X, d)\) which obeys axioms (bcd) of \cref{ii:1.1.2}, but not (a).
    \item Give an example of a pair \((X, d)\) which obeys axioms (acd) of \cref{ii:1.1.2}, but not (b).
    \item Give an example of a pair \((X, d)\) which obeys axioms (abd) of \cref{ii:1.1.2}, but not (c).
    \item Give an example of a pair \((X, d)\) which obeys axioms (abc) of \cref{ii:1.1.2}, but not (d).
  \end{enumerate}
\end{ex}

\begin{proof}{(a)}
  Let \(X = \R\), and let \(d(x, y) = 1\) for all \(x, y \in \R\).
  Then \((X, d)\) does not satisfy \cref{ii:1.1.2}(a) but (bcd).
\end{proof}

\begin{proof}{(b)}
  Let \(X = \R\) and let \(d(x, y) = 0\) for all \(x, y \in \R\).
  Then \((X, d)\) does not satisfy \cref{ii:1.1.2}(b) but (acd).
\end{proof}

\begin{proof}{(c)}
  Let \(X = \set{1, 2}\), let \(x, y \in X\) and let \(d(x, y) = x^y\) if \(x \neq y\) and \(d(x, y) = 0\) if \(x = y\).
  Then \((X, d)\) does not satisfy \cref{ii:1.1.2}(c) but (abd).
\end{proof}

\begin{proof}{(d)}
  Let \(X = \R^+\), let \(x, y \in \R^+\) and let \(d(x, y) = \max(x, y)\) if \(x \neq y\) and \(d(x, y) = 0\) if \(x = y\).
  Then \((X, d)\) does not satisfy \cref{ii:1.1.2}(d) but (abc).
\end{proof}

\begin{ex}\label{ii:ex:1.1.4}
  Show that the pair \((Y, d|_{Y \times Y})\) defined in \cref{ii:1.1.5} is indeed a metric space.
\end{ex}

\begin{proof}
  Let \(x, y, z \in X\).
  Since \(Y \subseteq X\), we know that \(x, y, z \in X\).
  For identity:
  We have \(d|_{Y \times Y}(x, x) = d(x, x) = 0\).
  For positivity:
  If \(x \neq y\), then \(d|_{Y \times Y}(x, y) = d(x, y) > 0\).
  For symmetry:
  We have \(d|_{Y \times Y}(x, y) = d(x, y) = d(y, x) = d|_{Y \times Y}(y, x)\).
  For triangle inequality:
  We have \(d|_{Y \times Y}(x, z) = d(x, z) \leq d(x, y) + d(y, z) = d|_{Y \times Y}(x, y) + d|_{Y \times Y}(y, z)\).
  Thus, by \cref{ii:1.1.2} \((Y, d|_{Y \times Y})\) is a metric space.
\end{proof}

\begin{ex}\label{ii:ex:1.1.5}
  Let \(n \geq 1\), and let \(a_1, a_2, \dots, a_n\) and \(b_1, b_2, \dots, b_n\) be real numbers.
  Verify the identity
  \[
    \bigg(\sum_{i = 1}^n a_i b_i\bigg)^2 + \dfrac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n (a_i b_j - a_j b_i)^2 = \bigg(\sum_{i = 1}^n a_i^2\bigg) \bigg(\sum_{j = 1}^n b_j^2\bigg)
  \]
  and conclude the \emph{Cauchy-Schwarz inequality}
  \[
    \abs{\sum_{i = 1}^n a_i b_i} \leq \bigg(\sum_{i = 1}^n a_i^2\bigg)^{1 / 2} \bigg(\sum_{j = 1}^n b_j^2\bigg)^{1 / 2}.
  \]
  Then use the Cauchy-Schwarz inequality to prove the \emph{triangle inequality}
  \[
    \bigg(\sum_{i = 1}^n (a_i + b_i)^2\bigg)^{1 / 2} \leq \bigg(\sum_{i = 1}^n a_i^2\bigg)^{1 / 2} + \bigg(\sum_{j = 1}^n b_j^2\bigg)^{1 / 2}.
  \]
\end{ex}

\begin{proof}
  We first show the identity is true by induction on \(n\).
  For \(n = 0\), we have
  \[
    \bigg(\sum_{i = 1}^0 a_i b_i\bigg)^2 + \dfrac{1}{2} \sum_{i = 1}^0 \sum_{j = 1}^0 (a_i b_j - a_j b_i)^2 = 0
  \]
  and
  \[
    \bigg(\sum_{i = 1}^0 a_i^2\bigg) \bigg(\sum_{j = 1}^0 b_j^2\bigg) = 0
  \]
  so the base case holds.
  Suppose inductively that the identity is true for some \(n \geq 0\).
  Then for \(n + 1\), we have
  \begin{align*}
      & \bigg(\sum_{i = 1}^{n + 1} a_i b_i\bigg)^2 + \dfrac{1}{2} \sum_{i = 1}^{n + 1} \sum_{j = 1}^{n + 1} (a_i b_j - a_j b_i)^2                                                                                  \\
    = & \bigg(\sum_{i = 1}^n a_i b_i + a_{n + 1} b_{n + 1}\bigg)^2 + \dfrac{1}{2} \sum_{i = 1}^{n + 1} \sum_{j = 1}^{n + 1} (a_i b_j - a_j b_i)^2                                                                  \\
    = & \bigg(\sum_{i = 1}^n a_i b_i\bigg)^2 + 2 \bigg(\sum_{i = 1}^n a_i b_i\bigg) (a_{n + 1} b_{n + 1}) + (a_{n + 1} b_{n + 1})^2 + \dfrac{1}{2} \sum_{i = 1}^{n + 1} \sum_{j = 1}^{n + 1} (a_i b_j - a_j b_i)^2 \\
    = & \bigg(\sum_{i = 1}^n a_i b_i\bigg)^2 + 2 \bigg(\sum_{i = 1}^n a_i b_i\bigg) (a_{n + 1} b_{n + 1}) + (a_{n + 1} b_{n + 1})^2                                                                                \\
      & + \dfrac{1}{2} \sum_{i = 1}^{n + 1} \bigg(\sum_{j = 1}^n (a_i b_j - a_j b_i)^2 + (a_i b_{n + 1} - a_{n + 1} b_i)^2\bigg)                                                                                   \\
    = & \bigg(\sum_{i = 1}^n a_i b_i\bigg)^2 + 2 \bigg(\sum_{i = 1}^n a_i b_i\bigg) (a_{n + 1} b_{n + 1}) + (a_{n + 1} b_{n + 1})^2                                                                                \\
      & + \dfrac{1}{2} \sum_{i = 1}^{n + 1} \sum_{j = 1}^n (a_i b_j - a_j b_i)^2 + \dfrac{1}{2} \sum_{i = 1}^{n + 1} (a_i b_{n + 1} - a_{n + 1} b_i)^2                                                             \\
    = & \bigg(\sum_{i = 1}^n a_i b_i\bigg)^2 + 2 \bigg(\sum_{i = 1}^n a_i b_i\bigg) (a_{n + 1} b_{n + 1}) + (a_{n + 1} b_{n + 1})^2                                                                                \\
      & + \dfrac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n (a_i b_j - a_j b_i)^2 + \dfrac{1}{2} \sum_{j = 1}^n (a_{n + 1} b_j - a_j b_{n + 1})^2 + \dfrac{1}{2} \sum_{i = 1}^{n + 1} (a_i b_{n + 1} - a_{n + 1} b_i)^2   \\
    = & \bigg(\sum_{i = 1}^n a_i b_i\bigg)^2 + 2 \bigg(\sum_{i = 1}^n a_i b_i\bigg) (a_{n + 1} b_{n + 1}) + (a_{n + 1} b_{n + 1})^2                                                                                \\
      & + \dfrac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n (a_i b_j - a_j b_i)^2 + \dfrac{1}{2} \sum_{j = 1}^n (a_{n + 1} b_j - a_j b_{n + 1})^2 + \dfrac{1}{2} \sum_{i = 1}^n (a_i b_{n + 1} - a_{n + 1} b_i)^2         \\
    = & \bigg(\sum_{i = 1}^n a_i b_i\bigg)^2 + 2 \bigg(\sum_{i = 1}^n a_i b_i\bigg) (a_{n + 1} b_{n + 1}) + (a_{n + 1} b_{n + 1})^2                                                                                \\
      & + \dfrac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n (a_i b_j - a_j b_i)^2 + \sum_{i = 1}^n (a_{n + 1} b_i - a_i b_{n + 1})^2
  \end{align*}
  and
  \begin{align*}
      & \bigg(\sum_{i = 1}^{n + 1} a_i^2\bigg) \bigg(\sum_{j = 1}^{n + 1} b_j^2\bigg)                                                                                                             \\
    = & \bigg(\sum_{i = 1}^n a_i^2 + a_{n + 1}^2\bigg) \bigg(\sum_{j = 1}^n b_j^2 + b_{n + 1}^2\bigg)                                                                                             \\
    = & \bigg(\sum_{i = 1}^n a_i^2\bigg) \bigg(\sum_{j = 1}^n b_j^2\bigg) + a_{n + 1}^2 \bigg(\sum_{j = 1}^n b_j^2\bigg) + b_{n + 1}^2 \bigg(\sum_{i = 1}^n a_i^2\bigg) + (a_{n + 1} b_{n + 1})^2 \\
    = & \bigg(\sum_{i = 1}^n a_i^2\bigg) \bigg(\sum_{j = 1}^n b_j^2\bigg) + \bigg(\sum_{j = 1}^n a_{n + 1}^2 b_j^2\bigg) + \bigg(\sum_{i = 1}^n a_i^2 b_{n + 1}^2\bigg) + (a_{n + 1} b_{n + 1})^2 \\
    = & \bigg(\sum_{i = 1}^n a_i^2\bigg) \bigg(\sum_{j = 1}^n b_j^2\bigg) + \bigg(\sum_{j = 1}^n a_{n + 1}^2 b_j^2 - 2 a_{n + 1} b_j a_j b_{n + 1} + a_j^2 b_{n + 1}^2\bigg)                      \\
      & + \sum_{j = 1}^n 2 a_{n + 1} b_j a_j b_{n + 1} - \sum_{j = 1}^n a_j^2 b_{n + 1}^2 + \bigg(\sum_{i = 1}^n a_i^2 b_{n + 1}^2\bigg) + (a_{n + 1} b_{n + 1})^2                                \\
    = & \bigg(\sum_{i = 1}^n a_i^2\bigg) \bigg(\sum_{j = 1}^n b_j^2\bigg) + \bigg(\sum_{j = 1}^n (a_{n + 1} b_j - a_j b_{n + 1})^2\bigg)                                                          \\
      & + 2 \sum_{j = 1}^n \bigg(b_j a_j\bigg)(a_{n + 1} b_{n + 1}) + (a_{n + 1} b_{n + 1})^2.
  \end{align*}
  By the induction hypothesis we thus have
  \[
    \bigg(\sum_{i = 1}^{n + 1} a_i b_i\bigg)^2 + \dfrac{1}{2} \sum_{i = 1}^{n + 1} \sum_{j = 1}^{n + 1} (a_i b_j - a_j b_i)^2 = \bigg(\sum_{i = 1}^{n + 1} a_i^2\bigg) \bigg(\sum_{j = 1}^{n + 1} b_j^2\bigg)
  \]
  and this closes the induction.

  Next we show that Cauchy-Schwarz inequality is true.
  We have
  \begin{align*}
             & \bigg(\sum_{i = 1}^n a_i b_i\bigg)^2 + \dfrac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n (a_i b_j - a_j b_i)^2 = \bigg(\sum_{i = 1}^n a_i^2\bigg) \bigg(\sum_{j = 1}^n b_j^2\bigg) \\
    \implies & \bigg(\sum_{i = 1}^n a_i b_i\bigg)^2 \leq \bigg(\sum_{i = 1}^n a_i^2\bigg) \bigg(\sum_{j = 1}^n b_j^2\bigg)                                                                 \\
    \implies & \abs{\sum_{i = 1}^n a_i b_i} \leq \bigg(\sum_{i = 1}^n a_i^2\bigg)^{1 / 2} \bigg(\sum_{j = 1}^n b_j^2\bigg)^{1 / 2}.
  \end{align*}

  Finally we show that \(d_{l^2}\) satisfy triangle inequality.
  We have
  \begin{align*}
    \sum_{i = 1}^n (a_i + b_i)^2 & = \sum_{i = 1}^n (a_i^2 + 2 a_i b_i + b_i^2)                                                                                           \\
                                 & = \sum_{i = 1}^n a_i^2 + 2 \sum_{i = 1}^n a_i b_i + \sum_{i = 1}^n b_i^2                                                               \\
                                 & \leq \sum_{i = 1}^n a_i^2 + 2 \abs{\sum_{i = 1}^n a_i b_i} + \sum_{i = 1}^n b_i^2                                                      \\
                                 & \leq \sum_{i = 1}^n a_i^2 + 2 \bigg(\sum_{i = 1}^n a_i^2\bigg)^{1 / 2} \bigg(\sum_{j = 1}^n b_j^2\bigg)^{1 / 2} + \sum_{i = 1}^n b_i^2 \\
                                 & = \bigg(\bigg(\sum_{i = 1}^n a_i^2\bigg)^{1 / 2} + \bigg(\sum_{j = 1}^n b_j^2\bigg)^{1 / 2}\bigg)^2
  \end{align*}
  and thus
  \[
    \bigg(\sum_{i = 1}^n (a_i + b_i)^2\bigg)^{1 / 2} \leq \bigg(\sum_{i = 1}^n a_i^2\bigg)^{1 / 2} + \bigg(\sum_{j = 1}^n b_j^2\bigg)^{1 / 2}.
  \]
\end{proof}

\begin{ex}\label{ii:ex:1.1.6}
  Show that \((\R^n, d_{l^2})\) in \cref{ii:1.1.6} is indeed a metric space.
\end{ex}

\begin{proof}
  Let \(n \in \N\) and let \(x, y, z \in \R^n\).
  For each \(n \in \N\), we define \(I_n = \set{i \in \N : 1 \leq i \leq n}\).
  For each \(i \in I_n\), we define \(x_i, y_i, z_i\) to be the \(i^{\opTh}\) coordinate of \(x, y, z\), respectively.
  For identity:
  We have
  \[
    d_{l^2}(x, x) = \bigg(\sum_{i = 1}^n (x_i - x_i)^2\bigg)^{1 / 2} = 0.
  \]
  For positivity:
  If \(x \neq y\), then \(\exists j \in I_n\) such that \(x_j \neq y_j\).
  So
  \[
    d_{l^2}(x, y) = \bigg(\sum_{i = 1}^n (x_i - y_i)^2\bigg)^{1 / 2} \geq \big((x_j - y_j)^2\big)^{1 / 2} > 0.
  \]
  For symmetry:
  We have
  \[
    d_{l^2}(x, y) = \bigg(\sum_{i = 1}^n (x_i - y_i)^2\bigg)^{1 / 2} = \bigg(\sum_{i = 1}^n (y_i - x_i)^2\bigg)^{1 / 2} = d_{l^2}(y, x).
  \]
  For triangle inequality:
  We define \(a_i = x_i - y_i\) and \(b_i = y_i - z_i\) for each \(i \in I_n\).
  Then we have
  \begin{align*}
    d_{l^2}(x, z) & = \bigg(\sum_{i = 1}^n (x_i - z_i)^2\bigg)^{1 / 2}                                                    \\
                  & = \bigg(\sum_{i = 1}^n (x_i - y_i + y_i - z_i)^2\bigg)^{1 / 2}                                        \\
                  & = \bigg(\sum_{i = 1}^n (a_i + b_i)^2\bigg)^{1 / 2}                                                    \\
                  & \leq \bigg(\sum_{i = 1}^n a_i^2\bigg)^{1 / 2} + \bigg(\sum_{i = 1}^n b_i^2\bigg)^{1 / 2}              \\
                  & = \bigg(\sum_{i = 1}^n (x_i - y_i)^2\bigg)^{1 / 2} + \bigg(\sum_{i = 1}^n (y_i - z_i)^2\bigg)^{1 / 2} \\
                  & = d_{l^2}(x, y) + d_{l^2}(y, z).
  \end{align*}
  Thus, by \cref{ii:1.1.2} \((\R^n, d_{l^2})\) is a metric space.
\end{proof}

\begin{ex}\label{ii:ex:1.1.7}
  Show that \((\R^n, d_{l^1})\) in \cref{ii:1.1.7} is indeed a metric space.
\end{ex}

\begin{proof}
  Let \(n \in \N\) and let \(x, y, z \in \R^n\).
  For each \(n \in \N\), we define \(I_n = \set{i \in \N : 1 \leq i \leq n}\).
  For each \(i \in I_n\), we define \(x_i, y_i, z_i\) to be the \(i^{\opTh}\) coordinate of \(x, y, z\), respectively.
  For identity:
  We have
  \[
    d_{l^1}(x, x) = \sum_{i = 1}^n \abs{x_i - x_i} = 0.
  \]
  For positivity:
  If \(x \neq y\), then \(\exists j \in I_n\) such that \(x_j \neq y_j\).
  So
  \[
    d_{l^1}(x, y) = \sum_{i = 1}^n \abs{x_i - y_i} \geq \abs{x_j - y_j} > 0.
  \]
  For symmetry:
  We have
  \[
    d_{l^1}(x, y) = \sum_{i = 1}^n \abs{x_i - y_i} = \sum_{i = 1}^n \abs{y_i - x_i} = d_{l^1}(y, x).
  \]
  For triangle inequality:
  We have
  \begin{align*}
    d_{l^1}(x, z) & = \sum_{i = 1}^n \abs{x_i - z_i}                        \\
                  & = \sum_{i = 1}^n \abs{x_i - y_i + y_i - z_i}            \\
                  & \leq \sum_{i = 1}^n (\abs{x_i - y_i} + \abs{y_i - z_i}) \\
                  & = d_{l^1}(x, y) + d_{l^1}(y, z).
  \end{align*}
  Thus, by \cref{ii:1.1.2} \((\R^n, d_{l^1})\) is a metric space.
\end{proof}

\begin{ex}\label{ii:ex:1.1.8}
  Prove the two inequalities
  \[
    d_{l^2}(x, y) \leq d_{l^1}(x, y) \leq \sqrt{n} d_{l^2}(x, y)
  \]
  for all \(x, y \in \R^n\).
\end{ex}

\begin{proof}
  Let \(n \in \N\) and let \(x, y \in \R^n\).
  For each \(n \in \N\), we define \(I_n = \set{i \in \N : 1 \leq i \leq n}\).
  For each \(i \in I_n\), we define \(x_i, y_i\) to be the \(i^{\opTh}\) coordinate of \(x, y\), respectively.
  We have
  \begin{align*}
    \big(d_{l^2}(x, y)\big)^2 & = \sum_{i = 1}^n (x_i - y_i)^2                                                                                                                                           \\
                              & = \sum_{i = 1}^n \abs{x_i - y_i}^2                                                                                                                                       \\
                              & \leq \sum_{i = 1}^n \abs{x_i - y_i}^2 + \sum_{i = 1}^n \Bigg(\abs{x_i - y_i} \bigg(\sum_{j = 1}^{i - 1} \abs{x_j - y_j} + \sum_{j = i + 1}^n \abs{x_j - y_j}\bigg)\Bigg) \\
                              & = \sum_{i = 1}^n \Bigg(\abs{x_i - y_i} \bigg(\sum_{j = 1}^{i - 1} \abs{x_j - y_j} + \abs{x_i - y_i} + \sum_{j = i + 1}^n \abs{x_j - y_j}\bigg)\Bigg)                     \\
                              & = \bigg(\sum_{i = 1}^n \abs{x_i - y_i}\bigg) \bigg(\sum_{j = 1}^n \abs{x_j - y_j}\bigg)                                                                                  \\
                              & = \bigg(\sum_{i = 1}^n \abs{x_i - y_i}\bigg)^2                                                                                                                           \\
                              & = \big(d_{l^1}(x, y)\big)^2
  \end{align*}
  and thus \(d_{l^2}(x, y) \leq d_{l^1}(x, y)\).
  Now let \(a_i = \abs{x_i - y_i}\) and \(b_i = 1\) for each \(i \in I_n\).
  Then we have
  \begin{align*}
    d_{l^1}(x, y) & = \sum_{i = 1}^n \abs{x_i - y_i}                                                                                  \\
                  & = \abs{\sum_{i = 1}^n \abs{x_i - y_i}}                                                                            \\
                  & = \abs{\sum_{i = 1}^n a_i b_i}                                                                                    \\
                  & \leq \bigg(\sum_{i = 1}^n a_i^2\bigg)^{1 / 2} \bigg(\sum_{i = 1}^n b_i^2\bigg)^{1 / 2}      &  & \by{ii:ex:1.1.5} \\
                  & = \bigg(\sum_{i = 1}^n \abs{x_i - y_i}^2\bigg)^{1 / 2} \bigg(\sum_{i = 1}^n 1\bigg)^{1 / 2}                       \\
                  & = \sqrt{n} d_{l^2}(x, y).
  \end{align*}
  Combining the results we have
  \[
    d_{l^2}(x, y) \leq d_{l^1}(x, y) \leq \sqrt{n} d_{l^2}(x, y).
  \]
\end{proof}

\begin{ex}\label{ii:ex:1.1.9}
  Show that \((\R^n, d_{l^\infty})\) in \cref{ii:1.1.9} is indeed a metric space.
\end{ex}

\begin{proof}
  Let \(n \in \N\) and let \(x, y, z \in \R^n\).
  For each \(n \in \N\), we define \(I_n = \set{i \in \N : 1 \leq i \leq n}\).
  For each \(i \in I_n\), we define \(x_i, y_i, z_i\) to be the \(i^{\opTh}\) coordinate of \(x, y, z\), respectively.
  For identity:
  We have
  \[
    d_{l^\infty}(x, x) = \sup \set{\abs{x_i - x_i} : i \in I_n} = 0.
  \]
  For positivity:
  If \(x \neq y\), then \(\exists j \in \N\) and \(1 \leq j \leq n\) such that \(x_j \neq y_j\).
  So
  \[
    d_{l^\infty}(x, y) = \sup \set{\abs{x_i - y_i} : i \in I_n} \geq \abs{x_j - y_j} > 0.
  \]
  For symmetry:
  We have
  \[
    d_{l^\infty}(x, y) = \sup \set{\abs{x_i - y_i} : i \in I_n} = \sup \set{\abs{y_i - x_i} : i \in I_n} = d_{l^\infty}(y, x).
  \]
  For triangle inequality:
  We have
  \begin{align*}
    d_{l^\infty}(x, z) & = \sup \set{\abs{x_i - z_i} : i \in I_n}                                             \\
                       & = \sup \set{\abs{x_i - y_i + y_i - z_i} : i \in I_n}                                 \\
                       & \leq \sup \set{\abs{x_i - y_i} + \abs{y_i - z_i} : i \in I_n}                        \\
                       & \leq \sup \set{\abs{x_i - y_i} : i \in I_n} + \sup \set{\abs{y_i - z_i} : i \in I_n} \\
                       & = d_{l^\infty}(x, y) + d_{l^\infty}(y, z).
  \end{align*}
  Thus, by \cref{ii:1.1.2} \((\R^n, d_{l^\infty})\) is a metric space.
\end{proof}

\begin{ex}\label{ii:ex:1.1.10}
  Prove the two inequalities
  \[
    \dfrac{1}{\sqrt{n}} d_{l^2}(x, y) \leq d_{l^\infty}(x, y) \leq d_{l^2}(x, y)
  \]
  for all \(x, y \in \R^n\).
\end{ex}

\begin{proof}
  Let \(n \in \N\) and let \(x, y \in \R^n\).
  For each \(n \in \N\), we define \(I_n = \set{i \in \N : 1 \leq i \leq n}\).
  For each \(i \in I_n\), we define \(x_i, y_i\) to be the \(i^{\opTh}\) coordinate of \(x, y\), respectively.
  Since
  \begin{align*}
    \dfrac{1}{\sqrt{n}} d_{l^2}(x, y) & = \dfrac{1}{\sqrt{n}} \bigg(\sum_{i = 1}^n (x_i - y_i)^2\bigg)^{1 / 2}                                         \\
                                      & \leq \dfrac{1}{\sqrt{n}} \bigg(\sum_{i = 1}^n \big(\sup \set{\abs{x_i - y_i} : i \in I_n}\big)^2\bigg)^{1 / 2} \\
                                      & = \dfrac{1}{\sqrt{n}} \Big(n \big(\sup \set{\abs{x_i - y_i} : i \in I_n}\big)^2\Big)^{1 / 2}                   \\
                                      & = \sup \set{\abs{x_i - y_i} : i \in I_n}                                                                       \\
                                      & = d_{l^\infty}(x, y)                                                                                           \\
                                      & = \Big(\big(\sup \set{\abs{x_i - y_i} : i \in I_n}\big)^2\Big)^{1 / 2}                                         \\
                                      & \leq \bigg(\sum_{i = 1}^n (x_i - y_i)^2\bigg)^{1 / 2}                                                          \\
                                      & = d_{l^2}(x, y),
  \end{align*}
  we have
  \[
    \dfrac{1}{\sqrt{n}} d_{l^2}(x, y) \leq d_{l^\infty}(x, y) \leq d_{l^2}(x, y).
  \]
\end{proof}

\begin{ex}\label{ii:ex:1.1.11}
  Show that \((X, d_{\text{disc}})\) in \cref{ii:1.1.11} is indeed a metric space.
\end{ex}

\begin{proof}
  Let \(x, y, z \in X\).
  For identity:
  We have
  \[
    d_{\text{disc}}(x, x) = 0.
  \]
  For positivity:
  If \(x \neq y\), then we have
  \[
    d_{\text{disc}}(x, y) = 1 > 0.
  \]
  For symmetry:
  We have
  \[
    x = y \iff d_{\text{disc}}(x, y) = 0 = d_{\text{disc}}(y, x)
  \]
  and
  \[
    x \neq y \iff d_{\text{disc}}(x, y) = 1 = d_{\text{disc}}(y, x).
  \]
  For triangle inequality:
  We have
  \[
    x = z \iff d_{\text{disc}}(x, z) = 0 \leq d_{\text{disc}}(x, y) + d_{\text{disc}}(y, z)
  \]
  and
  \[
    x \neq z \iff d_{\text{disc}}(x, z) = 1 \leq d_{\text{disc}}(x, y) + d_{\text{disc}}(y, z).
  \]
  Thus, by \cref{ii:1.1.2} \((X, d_{\text{disc}})\) is a metric space.
\end{proof}

\begin{ex}\label{ii:ex:1.1.12}
  Prove \cref{ii:1.1.18}.
\end{ex}

\begin{proof}
  See \cref{ii:1.1.18}.
\end{proof}

\begin{ex}\label{ii:ex:1.1.13}
  Prove \cref{ii:1.1.19}.
\end{ex}

\begin{proof}
  See \cref{ii:1.1.19}.
\end{proof}

\begin{ex}\label{ii:ex:1.1.14}
  Prove \cref{ii:1.1.20}.
\end{ex}

\begin{proof}
  See \cref{ii:1.1.20}.
\end{proof}

\begin{ex}\label{ii:ex:1.1.15}
  Let
  \[
    X \coloneqq \set{(a_n)_{n = 0}^\infty : \sum_{n = 0}^\infty \abs{a_n} < \infty}
  \]
  be the space of absolutely convergent sequences. Define the \(l^1\) and \(l^\infty\) metrics
  on this space by
  \begin{align*}
    d_{l^1}\big((a_n)_{n = 0}^\infty, (b_n)_{n = 0}^\infty\big)      & \coloneqq \sum_{n = 0}^\infty \abs{a_n - b_n}; \\
    d_{l^\infty}\big((a_n)_{n = 0}^\infty, (b_n)_{n = 0}^\infty\big) & \coloneqq \sup_{n \in \N} \abs{a_n - b_n}.
  \end{align*}
  Show that these are both metrics on \(X\), but show that there exist sequences \(x^{(1)}, x^{(2)}, \dots\) of elements of \(X\) (i.e., sequences of sequences) which are convergent with respect to the \(d_{l^\infty}\) metric but not with respect to the \(d_{l^1}\) metric.
  Conversely, show that any sequence which converges in the \(d_{l^1}\) metric automatically converges in the \(d_{l^\infty}\) metric.
\end{ex}

\begin{proof}
  Let \((a_n)_{n = 0}^\infty, (b_n)_{n = 0}^\infty, (c_n)_{n = 0}^\infty \in X\).
  Since \(\sum_{n = 0}^\infty a_n\) and \(\sum_{n = 0}^\infty b_n\) converge absolutely, we know that
  \begin{align*}
    \sum_{n = 0}^\infty \abs{a_n} + \sum_{n = 0}^\infty \abs{b_n} & = \lim_{N \to \infty} \sum_{n = 0}^N \abs{a_n} + \lim_{N \to \infty} \sum_{n = 0}^N \abs{b_n} \\
                                                                  & = \lim_{N \to \infty} \sum_{n = 0}^N \big(\abs{a_n} + \abs{b_n}\big)                          \\
                                                                  & \geq \lim_{N \to \infty} \sum_{n = 0}^N \abs{a_n - b_n}                                       \\
                                                                  & = \sum_{n = 0}^\infty \abs{a_n - b_n}                                                         \\
                                                                  & \geq \sup_{n \in N} \abs{a_n - b_n}.
  \end{align*}
  Thus, both \(\sum_{n = 0}^\infty \abs{a_n - b_n}\) and \(\sup_{n \in N} \abs{a_n - b_n}\) are well-defined and finite.

  We first show that \((X, d_{l^1})\) and \((X, d_{l^\infty})\) are metric spaces.
  For identity:
  We have
  \[
    d_{l^1}\big((a_n)_{n = 0}^\infty, (a_n)_{n = 0}^\infty\big) = \sum_{n = 0}^\infty \abs{a_n - a_n} = 0
  \]
  and
  \[
    d_{l^1}\big((a_n)_{n = 0}^\infty, (a_n)_{n = 0}^\infty\big) = \sup_{n \in \N} \abs{a_n - a_n} = 0.
  \]
  For positivity:
  If \((a_n)_{n = 0}^\infty \neq (b_n)_{n = 0}^\infty\), then \(\exists N \in \N\) such that \(a_N \neq b_N\).
  So we have
  \[
    d_{l^1}\big((a_n)_{n = 0}^\infty, (b_n)_{n = 0}^\infty\big) = \sum_{n = 0}^\infty \abs{a_n - b_n} \geq \abs{a_N - b_N} > 0
  \]
  and
  \[
    d_{l^\infty}\big((a_n)_{n = 0}^\infty, (b_n)_{n = 0}^\infty\big) = \sup_{n \in \N} \abs{a_n - b_n} \geq \abs{a_N - b_N} > 0.
  \]
  For symmetry:
  We have
  \[
    d_{l^1}\big((a_n)_{n = 0}^\infty, (b_n)_{n = 0}^\infty\big) = \sum_{n = 0}^\infty \abs{a_n - b_n} = \sum_{n = 0}^\infty \abs{b_n - a_n} = d_{l^1}\big((b_n)_{n = 0}^\infty, (a_n)_{n = 0}^\infty\big)
  \]
  and
  \[
    d_{l^\infty}\big((a_n)_{n = 0}^\infty, (b_n)_{n = 0}^\infty\big) = \sup_{n \in \N} \abs{a_n - b_n} = \sup_{n \in \N} \abs{b_n - a_n} = d_{l^\infty}\big((b_n)_{n = 0}^\infty, (a_n)_{n = 0}^\infty\big).
  \]
  For triangle inequality:
  We have
  \begin{align*}
    d_{l^1}\big((a_n)_{n = 0}^\infty, (c_n)_{n = 0}^\infty\big) & = \sum_{n = 0}^\infty \abs{a_n - c_n}                                                                                       \\
                                                                & = \sum_{n = 0}^\infty \abs{a_n - b_n + b_n - c_n}                                                                           \\
                                                                & \leq \sum_{n = 0}^\infty \abs{a_n - b_n} + \sum_{n = 0}^\infty \abs{b_n - c_n}                                              \\
                                                                & = d_{l^1}\big((a_n)_{n = 0}^\infty, (b_n)_{n = 0}^\infty\big) + d_{l^1}\big((b_n)_{n = 0}^\infty, (c_n)_{n = 0}^\infty\big)
  \end{align*}
  and
  \begin{align*}
    d_{l^\infty}\big((a_n)_{n = 0}^\infty, (c_n)_{n = 0}^\infty\big) & = \sup_{n \in \N} \abs{a_n - c_n}                                                                                                      \\
                                                                     & = \sup_{n \in \N} \abs{a_n - b_n + b_n - c_n}                                                                                          \\
                                                                     & \leq \sup_{n \in \N} (\abs{a_n - b_n} + \abs{b_n - c_n})                                                                               \\
                                                                     & \leq \sup_{n \in \N} \abs{a_n - b_n} + \sup_{n \in \N} \abs{b_n - c_n}                                                                 \\
                                                                     & = d_{l^\infty}\big((a_n)_{n = 0}^\infty, (b_n)_{n = 0}^\infty\big) + d_{l^\infty}\big((b_n)_{n = 0}^\infty, (c_n)_{n = 0}^\infty\big).
  \end{align*}
  Thus, by \cref{ii:1.1.2} \((X, d_{l^1})\) and \((X, d_{l^\infty})\) are metric spaces.

  Next we show that there exist sequences of elements of \(X\) which are convergent with respect to the \(d_{l^\infty}\) metric but not with respect to the \(d_{l^1}\) metric.
  Let \((x^{(k)})_{k = 1}^\infty\) be the sequence of sequence \((x_n^{(k)})_{n = 0}^\infty\) where
  \[
    x_n^{(k)} = \begin{dcases}
      0                             & \text{if } n = 0,    \\
      \dfrac{1}{n^2} + \dfrac{1}{k} & \text{if } n \leq k, \\
      \dfrac{1}{n^2}                & \text{if } n > k.
    \end{dcases}
  \]
  Then we know that \(\sum_{n = 0}^\infty \abs{x_n^{(k)}}\) is absolutely convergent for all \(k \in \N\) and \(k \geq 1\).
  Let \((y_n)_{n = 0}^\infty\) be a sequence where
  \[
    y_n = \begin{dcases}
      0              & \text{if } n = 0, \\
      \dfrac{1}{n^2} & \text{if } n > 0.
    \end{dcases}
  \]
  Then \(\sum_{n = 0}^\infty \abs{y_n}\) is also absolutely convergent.
  Now we show that \((x^{(k)})_{k = 1}^\infty\) converges to \((y_n)_{n = 0}^\infty\) with respect to \(d_{l^\infty}\).
  By \cref{ii:1.1.14} we need to show that
  \[
    \lim_{k \to \infty} d_{l^\infty}\big((x_n^{(k)})_{n = 0}^\infty, (y_n)_{n = 0}^\infty\big) = 0.
  \]
  We have
  \begin{align*}
    d_{l^\infty}\big((x_n^{(k)})_{n = 0}^\infty, (y_n)_{n = 0}^\infty\big) & = \sup_{n \in \N} \abs{x_n^{(k)} - y_n} \\
                                                                           & = \sup \set{0, \dfrac{1}{k}}            \\
                                                                           & = \dfrac{1}{k}
  \end{align*}
  and thus
  \[
    \lim_{k \to \infty} d_{l^\infty}\big((x_n^{(k)})_{n = 0}^\infty, (y_n)_{n = 0}^\infty\big) = \lim_{k \to \infty} \dfrac{1}{k} = 0.
  \]
  But we also have
  \begin{align*}
    d_{l^1}\big((x_n^{(k)})_{n = 0}^\infty, (y_n)_{n = 0}^\infty\big) & = \sum_{n = 0}^\infty \abs{x_n^{(k)} - y_n} \\
                                                                      & = \sum_{n = 1}^k \dfrac{1}{k}               \\
                                                                      & = 1
  \end{align*}
  and thus
  \[
    \lim_{k \to \infty} d_{l^1}\big((x_n^{(k)})_{n = 0}^\infty, (y_n)_{n = 0}^\infty\big) = \lim_{k \to \infty} 1 = 1 \neq 0.
  \]
  We conclude that \((x^{(k)})_{k = 1}^\infty\) converges to \((y_n)_{n = 0}^\infty\) with respect to \(d_{l^\infty}\) but not \(d_{l^1}\).

  Finally we show that any sequence which converges in the \(d_{l^1}\) metric automatically converges in the \(d_{l^\infty}\) metric.
  Suppose that \((x^{(k)})_{k = 1}^\infty\) converges to \((y_n)_{n = 0}^\infty\) with respect to \(d_{l^1}\).
  Since
  \begin{align*}
    d_{l^1}\big((x_n^{(k)})_{n = 0}^\infty, (y_n)_{n = 0}^\infty\big) & = \sum_{n = 0}^\infty \abs{x_n^{(k)} - y_n}                               \\
                                                                      & \geq \sup_{n \in N} \abs{x_n^{(k)} - y_n}                                 \\
                                                                      & = d_{l^\infty}\big((x_n^{(k)})_{n = 0}^\infty, (y_n)_{n = 0}^\infty\big),
  \end{align*}
  by squeeze test we have
  \begin{align*}
             & 0 \leq d_{l^\infty}\big((x_n^{(k)})_{n = 0}^\infty, (y_n)_{n = 0}^\infty\big) \leq d_{l^1}\big((x_n^{(k)})_{n = 0}^\infty, (y_n)_{n = 0}^\infty\big)                                                                     \\
    \implies & 0 = \lim_{k \to \infty} 0 \leq \lim_{k \to \infty} d_{l^\infty}\big((x_n^{(k)})_{n = 0}^\infty, (y_n)_{n = 0}^\infty\big) \leq \lim_{k \to \infty} d_{l^1}\big((x_n^{(k)})_{n = 0}^\infty, (y_n)_{n = 0}^\infty\big) = 0 \\
    \implies & \lim_{k \to \infty} d_{l^\infty}\big((x_n^{(k)})_{n = 0}^\infty, (y_n)_{n = 0}^\infty\big) = 0.
  \end{align*}
  Thus, any sequence which converges in the \(d_{l^1}\) metric automatically converges in the \(d_{l^\infty}\) metric.
\end{proof}

\begin{ex}\label{ii:ex:1.1.16}
  Let \((x_n)_{n = 1}^\infty\) and \((y_n)_{n = 1}^\infty\) be two sequences in a metric space \((X, d)\).
  Suppose that \((x_n)_{n = 1}^\infty\) converges to a point \(x \in X\), and \((y_n)_{n = 1}^\infty\) converges to a point \(y \in X\).
  Show that \(\lim_{n \to \infty} d(x_n, y_n) = d(x, y)\).
\end{ex}

\begin{proof}
  We have
  \begin{align*}
     & \lim_{n \to \infty} d(x_n, x) = 0;       &  & \by{ii:1.1.14} \\
     & \lim_{n \to \infty} d(y_n, y) = 0;       &  & \by{ii:1.1.14} \\
     & \lim_{n \to \infty} d(x, y)   = d(x, y).
  \end{align*}
  Since
  \begin{align*}
    d(x_n, y_n) & \leq d(x_n, x) + d(x, y_n)           &  & \by{ii:1.1.2}[d] \\
                & \leq d(x_n, x) + d(x, y) + d(y, y_n) &  & \by{ii:1.1.2}[d] \\
                & = d(x_n, x) + d(x, y) + d(y_n, y)    &  & \by{ii:1.1.2}[c]
  \end{align*}
  and
  \begin{align*}
    d(x, y) & \leq d(x, x_n) + d(x_n, y)               &  & \by{ii:1.1.2}[d] \\
            & \leq d(x, x_n) + d(x_n, y_n) + d(y_n, y) &  & \by{ii:1.1.2}[d] \\
            & = d(x_n, x) + d(x_n, y_n) + d(y_n, y),   &  & \by{ii:1.1.2}[c]
  \end{align*}
  we have
  \begin{align*}
             & \bigg(d(x_n, y_n) - d(x, y) \leq d(x_n, x) + d(y_n, y)\bigg)                                                                        \\
             & \land \bigg(d(x, y) - d(x_n, y_n) \leq d(x_n, x) + d(y_n, y)\bigg)                                                                  \\
    \implies & 0 \leq \abs{d(x_n, y_n) - d(x, y)} \leq d(x_n, x) + d(y_n, y)                                                                       \\
    \implies & \lim_{n \to \infty} 0 \leq \lim_{n \to \infty} \abs{d(x_n, y_n) - d(x, y)} \leq \lim_{n \to \infty} \big(d(x_n, x) + d(y_n, y)\big) \\
    \implies & 0 \leq \lim_{n \to \infty} \abs{d(x_n, y_n) - d(x, y)} \leq \lim_{n \to \infty} d(x_n, x) + \lim_{n \to \infty} d(y_n, y) = 0       \\
    \implies & \lim_{n \to \infty} \abs{d(x_n, y_n) - d(x, y)} = 0                                                                                 \\
    \implies & \lim_{n \to \infty} \big(d(x_n, y_n) - d(x, y)\big) = 0                                                                             \\
    \implies & \lim_{n \to \infty} d(x_n, y_n) - \lim_{n \to \infty} d(x, y) = 0                                                                   \\
    \implies & \lim_{n \to \infty} d(x_n, y_n) = \lim_{n \to \infty} d(x, y) = d(x, y).
  \end{align*}
\end{proof}
